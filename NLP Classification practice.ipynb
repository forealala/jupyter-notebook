{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955341aa",
   "metadata": {},
   "source": [
    "# LSTM 모델 사용 NLP Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37d4ad",
   "metadata": {},
   "source": [
    "1.1 Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5aaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, num_output, input_size, hidden_size, device):\n",
    "        super(ANN, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.outlayer = nn.Linear(hidden_size, num_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x).relu()\n",
    "        h = self.fc2(h).relu()\n",
    "        predict = sef.outlayer(h)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa770a",
   "metadata": {},
   "source": [
    "1.2 LSTM for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cbb9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, num_output, size_vocab, dim_embed, hidden_size, linear_size, num_layers, device):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        self.device = device #GPU\n",
    "        self.num_output = num_output #1\n",
    "        self.hidden_size = hidden_size #128\n",
    "        self.num_layers = num_layers #2\n",
    "        \n",
    "        self.embed = nn.Embedding(size_vocab, dim_embed)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = dim_embed, hidden_size = hidden_size,\n",
    "                           num_layers = num_layers, droupout = 0.3, bidirectional = True)\n",
    "        self.fclayer = nn.Linear(hidden_size, linear_size)\n",
    "        self.outlayer = nn.Linear(linear_size, num_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        scaler = 2 if self.lstm.bidirectional == True else 1\n",
    "        \n",
    "        emb = self.embed(x)\n",
    "        \n",
    "        h_state = Variable(torch.zeros(self.num_layers*scaler, emb.size(0),\n",
    "                                      self.hidden_size, requires_grad = True)).to(self.device)\n",
    "        c_state = Variable(torch.zeros(self.num_layers*scaler, emb.size(0),\n",
    "                                      self.hidden_size, requires_grad = True)).to(self.device)\n",
    "        \n",
    "        lstm_out, (h, c) = self.lstm(emb.transpose(1,0), (h_state, c_state))\n",
    "        h = h[-1] #important\n",
    "        h = self.fclayer(h).relu()\n",
    "        predict = self.outlayer(h)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102abbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
